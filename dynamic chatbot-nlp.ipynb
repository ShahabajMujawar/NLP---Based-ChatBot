{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cef921a",
   "metadata": {},
   "source": [
    "importing libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b32987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd68384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string  #punction , data preprocessing\n",
    "import numpy as np\n",
    "import wikipedia\n",
    "import io\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #data eccoding\n",
    "from sklearn.metrics.pairwise import cosine_similarity #similarity based response generation\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.stem import WordNetLemmatizer #5th step - datapreprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d946595",
   "metadata": {},
   "source": [
    "function for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c57d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(user_query):\n",
    "#     file = open('corpus.txt','r',errors='ignore')\n",
    "    corpus = wikipedia.summary(user_query) \n",
    "#     corpus = file.read()\n",
    "    sentence_tokens = nltk.sent_tokenize(corpus)\n",
    "    word_tokens = nltk.word_tokenize(corpus)\n",
    "    return sentence_tokens , word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "730efdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e160d",
   "metadata": {},
   "source": [
    "function for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b84e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370b651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemtokens(tokens):\n",
    "    lst=[] \n",
    "    for i in tokens: #every individual token have to be lemmatized\n",
    "        lst.append(lemmatizer.lemmatize(i))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f6b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove the punctucation  '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "#to remove the punctuation\n",
    "punct = dict((ord(i),None) for i in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7343acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmer(text):\n",
    "    #Tokenize\n",
    "    tokenized_text=nltk.word_tokenize(text.lower().translate(punct))\n",
    "    lemmatized_values = lemtokens(tokenized_text)\n",
    "    return lemmatized_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1413739a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'is', 'good', 'fruit']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmer('apple@ is good fruit!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a14c9c",
   "metadata": {},
   "source": [
    "#function for greeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c09e99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = ['hello','hi','hey','greetings']\n",
    "greeting_responses = ['i am a chatbot','hello','hi','whats up','hi there']\n",
    "\n",
    "def greeting(text):\n",
    "    for token in text.split():\n",
    "        if token.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d4c52",
   "metadata": {},
   "source": [
    "Function to generate response for the queries from the corpus!\n",
    "\n",
    "Data Encoding - TFIDF Similarity Metric - cosine similarity choosing the vector with maximum similarity in the corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "714750f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond(user_query):\n",
    "    bot_response=''\n",
    "    \n",
    "    #Tokenize\n",
    "    sent_tokens,word_tokens = tokenize(user_query)\n",
    "    sent_tokens.append(user_query)\n",
    "    \n",
    "    #Vectorizing\n",
    "    tfidf_obj = TfidfVectorizer(tokenizer=lemmer,stop_words=\"english\")\n",
    "    tfidf = tfidf_obj.fit_transform(sent_tokens)\n",
    "    \n",
    "    #cosine similarity\n",
    "    sim_values = cosine_similarity(tfidf[-1],tfidf) #cosine similarity of the last element with the entire list\n",
    "    \n",
    "    #selecting the response or token with max similarity\n",
    "    index = sim_values.argsort()[0][-2]\n",
    "    \n",
    "    flattened_sim = sim_values.flatten()\n",
    "    flattened_sim.sort()\n",
    "    \n",
    "    required_tfidf = flattened_sim[-2]\n",
    "    \n",
    "    if(required_tfidf==0):\n",
    "        bot_response += 'I cannot understand'\n",
    "        return bot_response\n",
    "    else:\n",
    "        bot_response += bot_response + sent_tokens[index]\n",
    "        return bot_response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b66e7",
   "metadata": {},
   "source": [
    "main() function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b54970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot\n",
      "hi\n",
      "Bot: whats up\n",
      "apple\n",
      "Bot:  I cannot understand\n",
      "apple mobile\n",
      "Bot:  Apple silicon is a series of system on a chip (SoC) and system in a package (SiP) processors designed by Apple Inc., mainly using the ARM architecture.\n",
      "exit\n",
      "Bot: Bye C u!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot\")\n",
    "flag = 1\n",
    "while(flag == 1):\n",
    "    user_query = input()\n",
    "    user_query = user_query.lower()\n",
    "    \n",
    "    #exit\n",
    "    if(user_query=='exit'):\n",
    "        flag=0\n",
    "        print('Bot: Bye C u!')\n",
    "    \n",
    "    else:\n",
    "        #greeting\n",
    "        if(greeting(user_query) != None):\n",
    "            print(\"Bot: \"+greeting(user_query))\n",
    "        \n",
    "        #general\n",
    "        else:\n",
    "            res = respond(user_query)\n",
    "            print(\"Bot: \",res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46228d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
